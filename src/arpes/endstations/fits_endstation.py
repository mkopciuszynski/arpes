"""Plugin for loading ARPES data exported in the FITS format from MAESTRO and similar software.

This module defines `FITSEndstation`, a plugin class that extends `EndstationBase`.
It implements logic specific to the non-standard FITS files generated by LabVIEW-based
data acquisition systems (e.g., MAESTRO, Lanzara Lab).

Key features include:

- Robust parsing of FITS headers and columns, including malformed or incomplete entries
- Heuristic reshaping of prematurely terminated scans
- Coordinate extraction from metadata tables
- Mapping of instrument-specific field names to standardized ARPES coordinates
- On-the-fly unit conversion (e.g., degrees â†’ radians)

Note:
This format is considered legacy, and future development may shift toward NeXus/HDF5.

Memo from RA: This class is no longer actively maintained.

See Also:
- `fits_utils.py` for shared parsing utilities
- `EndstationBase` for interface and behavior contracts
"""

from __future__ import annotations

import contextlib
import warnings
from logging import DEBUG, INFO
from pathlib import Path
from typing import TYPE_CHECKING, ClassVar

import numpy as np
import xarray as xr
from astropy.io import fits

from arpes.configuration.manager import config_manager
from arpes.debug import setup_logger
from arpes.helper.dict import rename_keys
from arpes.provenance import Provenance, provenance_from_file

from .base import EndstationBase
from .fits_utils import find_clean_coords

if TYPE_CHECKING:
    from _typeshed import Incomplete

    from arpes._typing import ScanDesc

LOGLEVELS = (DEBUG, INFO)
LOGLEVEL = LOGLEVELS[1]
logger = setup_logger(__name__, LOGLEVEL)


class FITSEndstation(EndstationBase):
    """Loads data from the .fits format produced by the MAESTRO software and derivatives.

    This ends up being somewhat complicated, because the FITS export is written in LabView and
    does not conform to the standard specification for the FITS archive format.

    Many of the intricacies here are in fact those shared between MAESTRO's format
    and the Lanzara Lab's format. Conrad does not foresee this as an issue, because it is
    unlikely that many other ARPES labs will adopt this data format moving forward, in
    light of better options derivative of HDF like the NeXuS format.

    Memo: RA would not maintain this class.
    """

    PREPPED_COLUMN_NAMES: ClassVar[dict[str, str]] = {
        "time": "time",
        "Delay": "delay-var",  # these are named thus to avoid conflicts with the
        "Sample-X": "cycle-var",  # underlying coordinates
        "Mira": "pump_power",
        # insert more as needed
    }

    SKIP_COLUMN_NAMES: ClassVar[set[str]] = {
        "Phi",
        "null",
        "X",
        "Y",
        "Z",
        "mono_eV",
        "Slit Defl",
        "Optics Stage",
        "Scan X",
        "Scan Y",
        "Scan Z",
        # insert more as needed
    }

    SKIP_COLUMN_FORMULAS: ClassVar = {
        lambda name: bool("beamview" in name or "IMAQdx" in name),
    }

    RENAME_KEYS: ClassVar[dict[str, str]] = {
        "Phi": "chi",
        "Beta": "beta",
        "Azimuth": "chi",
        "Pump_energy_uJcm2": "pump_fluence",
        "T0_ps": "t0_nominal",
        "W_func": "workfunction",
        "Slit": "slit",
        "LMOTOR0": "x",
        "LMOTOR1": "y",
        "LMOTOR2": "z",
        "LMOTOR3": "theta",
        "LMOTOR4": "beta",
        "LMOTOR5": "chi",
        "LMOTOR6": "alpha",
    }

    def resolve_frame_locations(self, scan_desc: ScanDesc | None = None) -> list[Path]:
        """Determines all files associated with a given scan.

        This function resolves the file location(s) based on the provided `scan_desc` dictionary.
        It looks for the "path" or "file" key in the `scan_desc` to determine the file location.
        If the file does not exist at the provided location, it will attempt to find it in the
        `config_manager.data_path` directory. If the file is still not found, a `RuntimeError` is
        raised.

        Args:
            scan_desc (ScanDesc | None): A dictionary containing scan metadata.
            It must include a "path" or "file" key specifying the location of the scan data file.

        Returns:
            list[Path]: A list containing the resolved file path(s).

        Raises:
            ValueError: If `scan_desc` is not provided or is `None`.
            RuntimeError: If the file cannot be found at the specified location or in the
                `config_manager.data_path` directory.
        """
        if scan_desc is None:
            msg = "Must pass dictionary as file scan_desc to all endstation loading code."
            raise ValueError(
                msg,
            )
        original_data_loc = scan_desc.get("path", scan_desc.get("file"))
        assert original_data_loc
        data_path = config_manager.data_path
        if not Path(original_data_loc).exists():
            if data_path is not None:
                original_data_loc = Path(data_path) / original_data_loc
            else:
                msg = "File not found"
                raise RuntimeError(msg)
        return [Path(original_data_loc)]

    def load_single_frame(  # noqa: PLR0915, PLR0912, C901
        self,
        frame_path: str | Path = "",
        scan_desc: ScanDesc | None = None,
        **kwargs: Incomplete,
    ) -> xr.Dataset:
        """Loads a scan from a single .fits file.

        This assumes the DAQ storage convention set by E. Rotenberg (possibly earlier authors)
        for the storage of ARPES data in FITS tables.

        This involves several complications:

        1. Hydrating/extracting coordinates from start/delta/n formats
        2. Extracting multiple scan regions
        3. Gracefully handling missing values
        4. Unwinding different scan conventions to common formats
        5. Handling early scan termination
        """
        if kwargs:
            logger.debug("load_single_frame: Any kwargs is not used at this level")
        # Use dimension labels instead of
        logger.debug("Opening FITS HDU list.")
        hdulist = fits.open(frame_path, ignore_missing_end=True)
        primary_dataset_name = None

        # Clean the header because sometimes out LabView produces improper FITS files
        for i in range(len(hdulist)):
            # This looks a little stupid, but because of confusing astropy internals actually works
            hdulist[i].header["UN_0_0"] = ""  # TODO: This card is broken, this is not a good fix
            del hdulist[i].header["UN_0_0"]
            hdulist[i].header["UN_0_0"] = ""
            if "TTYPE2" in hdulist[i].header and hdulist[i].header["TTYPE2"] == "Delay":
                logger.debug("Using ps delay units. This looks like an ALG main chamber scan.")
                hdulist[i].header["TUNIT2"] = ""
                del hdulist[i].header["TUNIT2"]
                hdulist[i].header["TUNIT2"] = "ps"

            logger.debug(f"HDU {i}: Attempting to fix FITS errors.")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                hdulist[i].verify("fix+warn")
                hdulist[i].header.update()
            # This actually requires substantially more work because it is lossy to information
            # on the unit that was encoded

        hdu = hdulist[1]
        scan_desc = scan_desc or {}
        attrs = scan_desc.pop("note", scan_desc)
        attrs.update(dict(hdulist[0].header))  # type: ignore  # noqa: PGH003

        drop_attrs = ["COMMENT", "HISTORY", "EXTEND", "SIMPLE", "SCANPAR", "SFKE_0"]
        for dropped_attr in drop_attrs:
            if dropped_attr in attrs:
                del attrs[dropped_attr]  # type: ignore  # noqa: PGH003

        built_coords, dimensions, real_spectrum_shape = find_clean_coords(
            hdu,
            attrs,  # type: ignore  # noqa: PGH003
            mode="MC",
        )
        logger.debug("Recovered coordinates from FITS file.")

        attrs = rename_keys(attrs, self.RENAME_KEYS)  # type: ignore  # noqa: PGH003
        scan_desc = rename_keys(scan_desc, self.RENAME_KEYS)  # type: ignore  # noqa: PGH003

        def clean_key_name(k: str) -> str:
            if "#" in k:
                k = k.replace("#", "num")
            return k

        attrs = {clean_key_name(k): v for k, v in attrs.items()}
        scan_desc = {clean_key_name(k): v for k, v in scan_desc.items()}  # type: ignore  # noqa: PGH003

        # don't have phi because we need to convert pixels first
        deg_to_rad_coords = {"beta", "theta", "chi"}

        # convert angular attributes to radians
        for coord_name in deg_to_rad_coords:
            if coord_name in attrs:
                with contextlib.suppress(TypeError, ValueError):
                    attrs[coord_name] = np.deg2rad(float(attrs[coord_name]))

            if coord_name in scan_desc:
                with contextlib.suppress(TypeError, ValueError):
                    scan_desc[coord_name] = np.deg2rad(float(scan_desc[coord_name]))  # type: ignore  # noqa: PGH003

        data_vars = {}

        all_names = hdu.columns.names
        n_spectra = len([n for n in all_names if "Fixed_Spectra" in n or "Swept_Spectra" in n])
        for column_name in hdu.columns.names:
            # we skip some fixed set of the columns, such as the one dimensional axes, as well as
            # things that are too tricky to load at the moment, like the microscope images from
            # MAESTRO
            should_skip = False
            if column_name in self.SKIP_COLUMN_NAMES:
                should_skip = True

            for formula in self.SKIP_COLUMN_FORMULAS:
                if formula(column_name):
                    should_skip = True

            if should_skip:
                continue

            # the hemisphere axis is handled below
            dimension_for_column = dimensions[column_name]
            column_shape = real_spectrum_shape[column_name]

            column_display = self.PREPPED_COLUMN_NAMES.get(column_name, column_name)
            if "Fixed_Spectra" in column_display:
                if n_spectra == 1:
                    column_display = "spectrum"
                else:
                    column_display = "spectrum" + "-" + column_display.split("Fixed_Spectra")[1]

            if "Swept_Spectra" in column_display:
                if n_spectra == 1:
                    column_display = "spectrum"
                else:
                    column_display = "spectrum" + "-" + column_display.split("Swept_Spectra")[1]

            # sometimes if a scan is terminated early it can happen that the sizes do not match the
            # expected value as an example, if a beta map is supposed to have 401 slices, it might
            # end up having only 260 if it were terminated early
            # If we are confident in our parsing code above, we can handle this case and take a
            # subset of the coords so that the data matches
            try:
                resized_data = hdu.data.columns[column_name].array.reshape(column_shape)
            except ValueError:
                # if we could not resize appropriately, we will try to reify the shapes together
                rest_column_shape = column_shape[1:]
                n_per_slice = int(np.prod(rest_column_shape))
                total_shape = hdu.data.columns[column_name].array.shape
                total_n = np.prod(total_shape)

                n_slices = total_n // n_per_slice
                # if this isn't true, we can't recover
                data_for_resize = hdu.data.columns[column_name].array
                if total_n // n_per_slice != total_n / n_per_slice:
                    # the last slice was in the middle of writing when something hit the fan
                    # we need to infer how much of the data to read, and then repeat the above
                    # we need to cut the data

                    # This can happen when the labview crashes during data collection,
                    # we use column_shape[1] because of the row order that is used in the FITS file
                    data_for_resize = data_for_resize[
                        0 : (total_n // n_per_slice) * column_shape[1]
                    ]
                    warning_msg = "Column {} was in the middle of slice when DAQ stopped."
                    warning_msg += "Throwing out incomplete slice..."
                    warnings.warn(
                        warning_msg.format(
                            column_name,
                        ),
                        stacklevel=2,
                    )

                column_shape = list(column_shape)
                column_shape[0] = n_slices

                try:
                    resized_data = data_for_resize.reshape(column_shape)
                except Exception:
                    logger.exception(
                        "Found an error in resized_data=data_for_resize.rechape(column_shape)",
                    )
                    # sometimes for whatever reason FITS errors and cannot read the data
                    continue

                # we also need to adjust the coordinates
                altered_dimension = dimension_for_column[0]
                built_coords[altered_dimension] = built_coords[altered_dimension][:n_slices]

            data_vars[column_display] = xr.DataArray(
                resized_data,
                coords={k: c for k, c in built_coords.items() if k in dimension_for_column},
                dims=dimension_for_column,
                attrs=attrs,
            )

        def prep_spectrum(data: xr.DataArray) -> xr.DataArray:
            # don't do center pixel inference because the main chamber
            # at least consistently records the offset from the edge
            # of the recorded window
            if "pixel" in data.coords:
                phi_axis = data.coords["pixel"].values * np.deg2rad(1 / 10)

                if "pixel" in data.coords:
                    data = data.rename(pixel="phi")

                data = data.assign_coords(phi=phi_axis)

            # Always attach provenance
            provenance_context: Provenance = {
                "what": "Loaded MC dataset from FITS.",
                "by": "load_MC",
            }
            provenance_from_file(data, str(frame_path), provenance_context)

            return data

        if "spectrum" in data_vars:
            data_vars["spectrum"] = prep_spectrum(data_vars["spectrum"])

        # adjust angular coordinates
        built_coords = {
            k: np.deg2rad(c) if k in deg_to_rad_coords else c for k, c in built_coords.items()
        }

        logger.debug("Stitching together xr.Dataset.")
        return xr.Dataset(
            {
                f"safe-{name}" if name in data_var.coords else name: data_var
                for name, data_var in data_vars.items()
            },
            attrs={**scan_desc, "dataset_name": primary_dataset_name},
        )
